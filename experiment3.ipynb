{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and prep the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(\"2019-2021_filtered_data.csv\")\n",
    "\n",
    "data['crash_datetime_y'] = pd.to_datetime(data['crash_datetime_y'])\n",
    "data['year'] = data['crash_datetime_y'].dt.year\n",
    "data['month'] = data['crash_datetime_y'].dt.month\n",
    "data['day'] = data['crash_datetime_y'].dt.day\n",
    "data['hour'] = data['crash_datetime_y'].dt.hour\n",
    "data = data.drop(columns=['crash_datetime_y'])\n",
    "\n",
    "zero_variance_cols = data.var(axis=0) == 0\n",
    "data = data.loc[:, ~zero_variance_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "numeric_columns = ['lat', 'long']\n",
    "categorical_columns = [\n",
    "    'crash_severity_id_x', 'vehicle_maneuver_id', 'vehicle_contrib_circum_id',\n",
    "    'extent_deformity_id', 'most_damaged_area_id', 'area_init_impact_id',\n",
    "    'most_harmful_event_id', 'event_sequence_1_id', 'event_sequence_2_id',\n",
    "    'event_sequence_3_id', 'event_sequence_4_id', 'crash_severity_id_y',\n",
    "    'first_harmful_event_id', 'roadway_contrib_circum_id'\n",
    "]\n",
    "\n",
    "lat_long_targets = data[numeric_columns]\n",
    "X_data = data.drop(columns=numeric_columns)\n",
    "\n",
    "coords = lat_long_targets[['lat', 'long']]\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # Adjust n_clusters as needed\n",
    "data['cluster_id'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# Add cluster_id to feature data\n",
    "X_data['cluster_id'] = data['cluster_id']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_data[col] = le.fit_transform(X_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X = pd.DataFrame(X_scaled, columns=X_data.columns)\n",
    "\n",
    "lat_long_scaler = RobustScaler()\n",
    "lat_long_scaled = lat_long_scaler.fit_transform(lat_long_targets)\n",
    "\n",
    "# Prepare final datasets\n",
    "X_data = X\n",
    "y_labels = {col: lat_long_scaled[:, i] for i, col in enumerate(['lat', 'long'])}\n",
    "\n",
    "# Debugging information\n",
    "print(\"NaNs in features:\", X_data.isna().sum().sum())\n",
    "print(\"NaNs in targets:\", pd.DataFrame(lat_long_scaled).isna().sum().sum())\n",
    "print(\"Infinities in features:\", np.isinf(X_data).sum().sum())\n",
    "print(\"Infinities in targets:\", np.isinf(lat_long_scaled).sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_data = X.copy()\n",
    "for key, value in y_labels.items():\n",
    "    combined_data[key] = value\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(combined_data):\n",
    "    train_data = combined_data.iloc[train_index]\n",
    "    test_data = combined_data.iloc[test_index]\n",
    "    X_train = train_data[X.columns]\n",
    "    X_test = test_data[X.columns]\n",
    "    y_train = {key: train_data[key].values for key in y_labels.keys()}\n",
    "    y_test = {key: test_data[key].values for key in y_labels.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for Lat/Lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_mse = []\n",
    "fold_mae = []\n",
    "fold_results = []\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_data)):\n",
    "    print(f\"Training on fold {fold + 1}/{n_splits}...\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_data.iloc[train_idx], X_data.iloc[val_idx]\n",
    "    y_train_fold = {key: y_labels[key][train_idx] for key in y_labels.keys()}\n",
    "    y_val_fold = {key: y_labels[key][val_idx] for key in y_labels.keys()}\n",
    "\n",
    "    # Define the model\n",
    "    inputs = Input(shape=(X_data.shape[1],))\n",
    "    x = Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "\n",
    "    # Separate branches for latitude and longitude\n",
    "    lat_branch = Dense(25, activation='relu')(x)\n",
    "    long_branch = Dense(25, activation='relu')(x)\n",
    "\n",
    "    lat_output = Dense(1, activation='linear', name='lat')(lat_branch)\n",
    "    long_output = Dense(1, activation='linear', name='long')(long_branch)\n",
    "\n",
    "    lat_long_pred_model = Model(inputs=inputs, outputs=[lat_output, long_output])\n",
    "\n",
    "    # Compile the model\n",
    "    lat_long_pred_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={'lat': Huber(delta=1.0), 'long': Huber(delta=1.0)},\n",
    "        metrics={'lat': 'mae', 'long': 'mae'}\n",
    "    )\n",
    "\n",
    "    # Set callbacks for training\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    history = lat_long_pred_model.fit(\n",
    "        X_train_fold,\n",
    "        [y_train_fold['lat'], y_train_fold['long']],\n",
    "        validation_data=(X_val_fold, [y_val_fold['lat'], y_val_fold['long']]),\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_pred_lat, y_pred_long = lat_long_pred_model.predict(X_val_fold)\n",
    "\n",
    "    # Convert predictions to binary (or discrete categories as needed)\n",
    "    y_pred_lat_binary = (y_pred_lat > 0.5).astype(int)  # Example threshold\n",
    "    y_pred_long_binary = (y_pred_long > 0.5).astype(int)\n",
    "\n",
    "    y_val_lat_binary = (y_val_fold['lat'] > 0.5).astype(int)  # Assuming thresholding logic for labels\n",
    "    y_val_long_binary = (y_val_fold['long'] > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics for latitude predictions\n",
    "    accuracy_lat = accuracy_score(y_val_lat_binary, y_pred_lat_binary)\n",
    "    precision_lat = precision_score(y_val_lat_binary, y_pred_lat_binary)\n",
    "    f1_lat = f1_score(y_val_lat_binary, y_pred_lat_binary)\n",
    "\n",
    "    # Calculate metrics for longitude predictions\n",
    "    accuracy_long = accuracy_score(y_val_long_binary, y_pred_long_binary)\n",
    "    precision_long = precision_score(y_val_long_binary, y_pred_long_binary)\n",
    "    f1_long = f1_score(y_val_long_binary, y_pred_long_binary)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Metrics:\")\n",
    "    print(f\"  Latitude: Accuracy={accuracy_lat:.4f}, Precision={precision_lat:.4f}, F1 Score={f1_lat:.4f}\")\n",
    "    print(f\"  Longitude: Accuracy={accuracy_long:.4f}, Precision={precision_long:.4f}, F1 Score={f1_long:.4f}\")\n",
    "\n",
    "    # Store fold results (extend this as needed for final averages)\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'lat_accuracy': accuracy_lat,\n",
    "        'lat_precision': precision_lat,\n",
    "        'lat_f1': f1_lat,\n",
    "        'long_accuracy': accuracy_long,\n",
    "        'long_precision': precision_long,\n",
    "        'long_f1': f1_long,\n",
    "    })\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Fold {fold + 1} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot MAE for latitude and longitude\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['lat_mae'], label='Training MAE (Lat)')\n",
    "    plt.plot(history.history['val_lat_mae'], label='Validation MAE (Lat)')\n",
    "    plt.plot(history.history['long_mae'], label='Training MAE (Long)')\n",
    "    plt.plot(history.history['val_long_mae'], label='Validation MAE (Long)')\n",
    "    plt.title(f'Fold {fold + 1} MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Compute average metrics across all folds\n",
    "mean_mse = np.mean(fold_mse)\n",
    "mean_mae = np.mean(fold_mae)\n",
    "\n",
    "print(f\"Average MSE across {n_splits} folds: {mean_mse:.4f}\")\n",
    "print(f\"Average MAE across {n_splits} folds: {mean_mae:.4f}\")\n",
    "\n",
    "# Boxplot for metrics across folds\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Fold': range(1, n_splits + 1),\n",
    "    'MSE': fold_mse,\n",
    "    'MAE': fold_mae\n",
    "})\n",
    "\n",
    "metrics_df.boxplot(column=['MSE', 'MAE'], grid=False, figsize=(8, 6))\n",
    "plt.title('Model Performance Across Folds')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print metrics table\n",
    "metrics_df.loc['Average'] = ['Average', mean_mse, mean_mae]\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
