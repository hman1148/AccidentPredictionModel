{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data and prep the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"2022-2023_filtered_data.csv\")\n",
    "\n",
    "# Convert datetime column and extract features\n",
    "data['crash_datetime_y'] = pd.to_datetime(data['crash_datetime_y'])\n",
    "data['year'] = data['crash_datetime_y'].dt.year\n",
    "data['month'] = data['crash_datetime_y'].dt.month\n",
    "data['day'] = data['crash_datetime_y'].dt.day\n",
    "data['hour'] = data['crash_datetime_y'].dt.hour\n",
    "data = data.drop(columns=['crash_datetime_y']) \n",
    "\n",
    "# Identify and drop zero-variance columns\n",
    "zero_variance_cols = data.var(axis=0) == 0\n",
    "data = data.loc[:, ~zero_variance_cols]\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Scale the feature data (z-normalization)\n",
    "scaler = RobustScaler()\n",
    "numeric_columns = ['lat', 'long']  # Targets that are numeric and continuous\n",
    "categorical_columns = [\n",
    "    'crash_severity_id_x', 'vehicle_maneuver_id', 'vehicle_contrib_circum_id',\n",
    "    'extent_deformity_id', 'most_damaged_area_id', 'area_init_impact_id',\n",
    "    'most_harmful_event_id', 'event_sequence_1_id', 'event_sequence_2_id',\n",
    "    'event_sequence_3_id', 'event_sequence_4_id', 'crash_severity_id_y',\n",
    "    'first_harmful_event_id', 'roadway_contrib_circum_id'\n",
    "]\n",
    "\n",
    "# Separate latitude and longitude as target variables\n",
    "lat_long_targets = data[numeric_columns]\n",
    "X_data = data.drop(columns=numeric_columns)  \n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_data[col] = le.fit_transform(X_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale the remaining features (excluding categorical columns, which are already label-encoded)\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X = pd.DataFrame(X_scaled, columns=X_data.columns)\n",
    "\n",
    "# Scale latitude and longitude targets\n",
    "lat_long_scaler = RobustScaler()\n",
    "lat_long_scaled = lat_long_scaler.fit_transform(lat_long_targets)\n",
    "\n",
    "# Prepare final features and targets\n",
    "X_data = X  # Features are the scaled dataframe\n",
    "y_labels = {col: lat_long_scaled[:, i] for i, col in enumerate(['lat', 'long'])}\n",
    "\n",
    "# Debugging step: Check for missing or infinite values\n",
    "print(\"NaNs in features:\", X_data.isna().sum().sum())\n",
    "print(\"NaNs in targets:\", pd.DataFrame(lat_long_scaled).isna().sum().sum())\n",
    "print(\"Infinities in features:\", np.isinf(X_data).sum().sum())\n",
    "print(\"Infinities in targets:\", np.isinf(lat_long_scaled).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y_labels for splitting\n",
    "combined_data = X.copy()\n",
    "for key, value in y_labels.items():\n",
    "    combined_data[key] = value\n",
    "\n",
    "# Split into training and test data\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.3, random_state=42)\n",
    "X_train = train_data[X.columns]\n",
    "X_test = test_data[X.columns]\n",
    "y_train = {key: train_data[key].values for key in y_labels.keys()}\n",
    "y_test = {key: test_data[key].values for key in y_labels.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for Lat/Lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the regression model for Latitude and Longitude\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "\n",
    "x = Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(inputs)\n",
    "x = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(x)\n",
    "lat_output = Dense(1, activation='linear', name='lat')(x)\n",
    "long_output = Dense(1, activation='linear', name='long')(x)\n",
    "\n",
    "lat_long_pred_model = Model(inputs=inputs, outputs=[lat_output, long_output])\n",
    "lat_long_pred_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001, clipvalue=1.0),  # Added gradient clipping\n",
    "    loss={'lat': 'mse', 'long': 'mse'},\n",
    "    metrics={'lat': 'mae', 'long': 'mae'}\n",
    ")\n",
    "\n",
    "# Train the regression model\n",
    "regression_history = lat_long_pred_model.fit(\n",
    "    X_train, [y_train['lat'], y_train['long']],\n",
    "    epochs=5, batch_size=32, validation_data=(X_test, [y_test['lat'], y_test['long']])\n",
    ")\n",
    "\n",
    "# Predict latitude and longitude on training and test sets\n",
    "predicted_lat_long_train = lat_long_pred_model.predict(X_train)\n",
    "predicted_lat_long_test = lat_long_pred_model.predict(X_test)\n",
    "\n",
    "X_train_classification = np.column_stack(predicted_lat_long_train)\n",
    "X_test_classification = np.column_stack(predicted_lat_long_test)\n",
    "\n",
    "# Plot training and validation loss (MSE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(regression_history.history['lat_loss'], label='Train MSE (lat)')\n",
    "plt.plot(regression_history.history['val_lat_loss'], label='Validation MSE (lat)')\n",
    "plt.plot(regression_history.history['long_loss'], label='Train MSE (long)')\n",
    "plt.plot(regression_history.history['val_long_loss'], label='Validation MSE (long)')\n",
    "plt.title('Mean Squared Error (MSE) Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(regression_history.history['lat_mae'], label='Train MAE (lat)')\n",
    "plt.plot(regression_history.history['val_lat_mae'], label='Validation MAE (lat)')\n",
    "plt.plot(regression_history.history['long_mae'], label='Train MAE (long)')\n",
    "plt.plot(regression_history.history['val_long_mae'], label='Validation MAE (long)')\n",
    "plt.title('Mean Absolute Error (MAE) Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Classification Evaluation\n",
    "# Define a threshold for binary classification\n",
    "classification_threshold = 0.5\n",
    "\n",
    "# Generate binary ground truth labels from the test set\n",
    "true_lat_class = (y_test['lat'] > classification_threshold).astype(int)\n",
    "true_long_class = (y_test['long'] > classification_threshold).astype(int)\n",
    "\n",
    "# Convert regression predictions to binary predictions\n",
    "predicted_lat_class = (predicted_lat_long_test[0] > classification_threshold).astype(int)\n",
    "predicted_long_class = (predicted_lat_long_test[1] > classification_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics for latitude\n",
    "lat_accuracy = accuracy_score(true_lat_class, predicted_lat_class)\n",
    "lat_precision = precision_score(true_lat_class, predicted_lat_class, zero_division=0)\n",
    "lat_f1 = f1_score(true_lat_class, predicted_lat_class, zero_division=0)\n",
    "\n",
    "# Calculate metrics for longitude\n",
    "long_accuracy = accuracy_score(true_long_class, predicted_long_class)\n",
    "long_precision = precision_score(true_long_class, predicted_long_class, zero_division=0)\n",
    "long_f1 = f1_score(true_long_class, predicted_long_class, zero_division=0)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Latitude Metrics:\")\n",
    "print(f\"  Accuracy: {lat_accuracy:.4f}\")\n",
    "print(f\"  Precision: {lat_precision:.4f}\")\n",
    "print(f\"  F1 Score: {lat_f1:.4f}\")\n",
    "\n",
    "print(\"\\nLongitude Metrics:\")\n",
    "print(f\"  Accuracy: {long_accuracy:.4f}\")\n",
    "print(f\"  Precision: {long_precision:.4f}\")\n",
    "print(f\"  F1 Score: {long_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classification model using predicted lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined classification model\n",
    "classification_inputs = Input(shape=(2, )) # Two inputs predicted lat/lng\n",
    "\n",
    "x = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(classification_inputs)\n",
    "x = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(x)\n",
    "\n",
    "# Create output layers for each categorical label\n",
    "classification_outputs = {}\n",
    "for label_name, encoder in data.keys().items(): \n",
    "    num_classes = len(encoder.classes_)\n",
    "    classification_outputs[label_name] = Dense(num_classes, activation='softmax', name=label_name)(x)\n",
    "    \n",
    "# Compile classification model\n",
    "classification_label_model = Model(inputs=classification_inputs, outputs=classification_outputs)\n",
    "classification_label_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss={label: 'sparse_categorical_crossentropy' for label in data.keys()},\n",
    "    metrics={label: 'accuracy' for label in data.keys()}\n",
    ")\n",
    "\n",
    "# Train classification model on the predicted lat/long inputs and categorical labels\n",
    "classification_label_model_history = classification_label_model.fit(\n",
    "    X_train_classification, {label: y_train[label] for label in data.keys()},\n",
    "    epochs=20, batch_size=32, validation_data=(X_test_classification, {label: y_test[label] for label in data.keys()})\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inputs = Input(shape=(X.shape[1],))\n",
    "\n",
    "\n",
    "# # Shared layers with reduced L2 regularization\n",
    "# x = Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(inputs)\n",
    "# x = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(x)\n",
    "# shared = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(x)\n",
    "\n",
    "# # Regression branch for lat/long (linear activation)\n",
    "# reg_branch = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(shared)\n",
    "# lat_output = Dense(1, activation='linear', name='lat')(reg_branch)\n",
    "# long_output = Dense(1, activation='linear', name='long')(reg_branch)\n",
    "\n",
    "# # Classification branch for categorical targets\n",
    "# class_branch = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')(shared)\n",
    "# outputs = {'lat': lat_output, 'long': long_output}\n",
    "\n",
    "# # Add classification outputs for each label\n",
    "# for label_name in label_encoders.keys():\n",
    "#     num_classes = len(label_encoders[label_name].classes_)\n",
    "#     outputs[label_name] = Dense(num_classes, activation='softmax', name=label_name)(class_branch)\n",
    "\n",
    "# # Compile the model with a lower learning rate and gradient clipping\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=0.0001, clipvalue=1.0),\n",
    "#     loss={**{'lat': 'mse', 'long': 'mse'}, **{k: 'sparse_categorical_crossentropy' for k in label_encoders.keys()}},\n",
    "#     metrics={**{'lat': 'mse', 'long': 'mse'}, **{k: 'accuracy' for k in label_encoders.keys()}}\n",
    "# )\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the training data gathered earlier to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_test, y_test) \n",
    "# )\n",
    "\n",
    "# # Show test results\n",
    "# test_results = model.evaluate(X_test, {key: y_test[key] for key in y_test})\n",
    "\n",
    "# # Plot training and validation accuracy\n",
    "# plt.plot(history.history['crash_severity_id_x_accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_crash_severity_id_x_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Accuracy for Crash Severity ID X')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre processing the routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bearing(pointA, pointB):\n",
    "    \n",
    "    # Calculate the direction from pointA to pointB\n",
    "    lat1, lng1 = np.radians(pointA)\n",
    "    lat2, lng2 = np.radians(pointB)\n",
    "    \n",
    "    delta_lng = lng2 - lng1 \n",
    "    x = np.sin(delta_lng) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - (np.sin(lat1) * np.cos(lat2) * np.cos(delta_lng))\n",
    "    \n",
    "    bearing = np.degrees(np.arctan2(x, y)) \n",
    "    return (bearing + 360) % 360  # Normalize to 0-360 degrees\n",
    "\n",
    "def preprocess_route(start_lat, start_lng, end_lat, end_lng, route_points):\n",
    "    \n",
    "    #Process the input route into a feature vector for the model\n",
    "    \n",
    "    full_route = [(start_lat, start_lng)] + route_points + [(end_lat, end_lng)]\n",
    "    \n",
    "    distances = []\n",
    "    bearings = []\n",
    "    \n",
    "    for index in range(len(full_route) - 1):\n",
    "        pointA = full_route[index]\n",
    "        pointB = full_route[index + 1]\n",
    "        \n",
    "        #Distance between points in miles\n",
    "        distance = geodesic(pointA, pointB).miles\n",
    "        distances.append(distance)\n",
    "        \n",
    "        # Bearing between points\n",
    "        bearing = calculate_bearing(pointA, pointB)\n",
    "        bearings.append(bearing)\n",
    "        \n",
    "    # convert lists to numpy arrays\n",
    "    distances = np.array(distances)\n",
    "    bearings = np.array(bearings)\n",
    "    \n",
    "    features = np.concatenate([distances, bearings])\n",
    "    # Reshape to 2D array, as required by model input (1 sample, n features)\n",
    "    return features.reshape(1, -1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input from a user for route prediction\n",
    "start_lat = 34.052235\n",
    "start_long = -118.243683\n",
    "end_lat = 40.712776\n",
    "end_long = -74.005974\n",
    "route_points = [\n",
    "    (36.169941, -115.139832),  # Example intermediate point\n",
    "    (39.739235, -104.990250)   # Another example intermediate point\n",
    "]\n",
    "\n",
    "# Process the route to generate features for the regression model\n",
    "route_features = preprocess_route(start_lat, start_long, end_lat, end_long, route_points)\n",
    "\n",
    "# Ensure that `route_features` shape matches the expected input shape for the regression model\n",
    "if route_features.shape[1] != X.shape[1]:\n",
    "    raise ValueError(f\"Feature vector shape {route_features.shape[1]} does not match expected input shape {X.shape[1]}\")\n",
    "\n",
    "# Predict lat and long using the regression model\n",
    "predicted_lat_long = lat_long_pred_model.predict(route_features)\n",
    "predicted_lat = predicted_lat_long[0][0]\n",
    "predicted_long = predicted_lat_long[1][0]\n",
    "\n",
    "# Combine predicted lat and long as input for the classification model\n",
    "predicted_lat_long_input = np.array([[predicted_lat, predicted_long]])\n",
    "\n",
    "# Predict categorical labels using the classification model\n",
    "classification_predictions = classification_label_model.predict(predicted_lat_long_input)\n",
    "\n",
    "# Decode and print categorical predictions\n",
    "decoded_predictions = {}\n",
    "for label_name, pred in classification_predictions.items():\n",
    "    decoded_predictions[label_name] = data[label_name].inverse_transform(np.argmax(pred, axis=1))\n",
    "\n",
    "# Display the final predicted accident location and details\n",
    "print(\"Predicted Accident Location:\")\n",
    "print(f\"Latitude: {predicted_lat}, Longitude: {predicted_long}\")\n",
    "\n",
    "print(\"Predicted Accident Details:\")\n",
    "for label_name, decoded_value in decoded_predictions.items():\n",
    "    print(f\"{label_name}: {decoded_value[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
